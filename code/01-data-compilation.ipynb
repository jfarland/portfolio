{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "Being able to accurately forecast the volatile demand for power, especially across low-voltage distribution grids, is increasingly important to providing sufficent energy supply. Paradoxically, it's at the low-voltage networks that are the most volatile, and yet the most critical to account for as more distributed energy resources (e.g., solar, storage) make their way onto the grid. From the grid operator's point of view, not only are lower voltage service points (e.g., customers) the revenue life blood of the business, having increased visibility into demand at such lower levels of aggregation helps to provide \n",
    "\n",
    "\n",
    "In this notebook, I explore looking at a [Kaggle data set](https://www.kaggle.com/jeanmidev/smart-meters-in-london) containing about 5,000 utility service points with time series data from advanced metering infrastructure (AMI). The context of this experiment is to simulate the needs of a utility or grid operator and generate accurate short-term (up to 7 days ahead) forecast. I explore possible approaches that involve\n",
    "\n",
    "- *Feature-based modeling approaches*\n",
    "- *Customer Segmentation + Clustering*\n",
    "- *Statistical methods, ML methods, and hybrid cross-learning approaches*\n",
    "\n",
    "Performance is evaluated using time-series cross-validation using a rolling 7-day forecast horizon.\n",
    "\n",
    "Finally, I show how this solution might be scaled and put into production.\n",
    "\n",
    "Keywords: *time series classification*, *energy forecasting*, *cross-learning*, *MLOps*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T22:16:35.257709Z",
     "start_time": "2021-10-09T22:16:19.760705Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "import tensorflow\n",
    "\n",
    "from tsfeatures import tsfeatures\n",
    "from tsfeatures import crossing_points, acf_features, stl_features, entropy, nonlinearity, stability\n",
    "import tsfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T22:16:35.312236Z",
     "start_time": "2021-10-09T22:16:35.262207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/portfolio/data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Project Working Directory\n",
    "project_path = '/workspace/portfolio'\n",
    "\n",
    "os.chdir(project_path)\n",
    "\n",
    "# Set Data Path\n",
    "data_path = f\"{project_path}/data\"\n",
    "data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data set we will use contains high-frequency measures of electricity demand usage across a panel of approximately 5,000 homes. It also contains customer metadata from the [ACORN](https://acorn.caci.co.uk/downloads/Acorn-User-guide.pdf) network, as well as atmospheric weather data from the DarkSky API (now owned by Apple and will be sunset by 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:21:07.773495Z",
     "start_time": "2021-10-09T00:20:04.202944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LCLid</th>\n",
       "      <th>day</th>\n",
       "      <th>hh_0</th>\n",
       "      <th>hh_1</th>\n",
       "      <th>hh_2</th>\n",
       "      <th>hh_3</th>\n",
       "      <th>hh_4</th>\n",
       "      <th>hh_5</th>\n",
       "      <th>hh_6</th>\n",
       "      <th>hh_7</th>\n",
       "      <th>...</th>\n",
       "      <th>hh_38</th>\n",
       "      <th>hh_39</th>\n",
       "      <th>hh_40</th>\n",
       "      <th>hh_41</th>\n",
       "      <th>hh_42</th>\n",
       "      <th>hh_43</th>\n",
       "      <th>hh_44</th>\n",
       "      <th>hh_45</th>\n",
       "      <th>hh_46</th>\n",
       "      <th>hh_47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAC000013</td>\n",
       "      <td>2012-06-22</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAC000013</td>\n",
       "      <td>2012-06-23</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAC000013</td>\n",
       "      <td>2012-06-25</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAC000013</td>\n",
       "      <td>2012-06-26</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAC000013</td>\n",
       "      <td>2012-06-27</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LCLid         day   hh_0   hh_1   hh_2   hh_3   hh_4   hh_5   hh_6  \\\n",
       "0  MAC000013  2012-06-22  0.127  0.113  0.076  0.109  0.120  0.096  0.113   \n",
       "1  MAC000013  2012-06-23  0.106  0.135  0.228  0.079  0.109  0.117  0.079   \n",
       "2  MAC000013  2012-06-25  0.099  0.119  0.169  0.136  0.109  0.090  0.132   \n",
       "3  MAC000013  2012-06-26  0.142  0.103  0.117  0.136  0.127  0.129  0.150   \n",
       "4  MAC000013  2012-06-27  0.126  0.128  0.175  0.163  0.119  0.103  0.128   \n",
       "\n",
       "    hh_7  ...  hh_38  hh_39  hh_40  hh_41  hh_42  hh_43  hh_44  hh_45  hh_46  \\\n",
       "0  0.148  ...  0.086  0.154  0.121  0.102  0.131  0.121  0.082  0.096  0.127   \n",
       "1  0.122  ...  0.107  0.113  0.106  0.077  0.119  0.126  0.151  0.135  0.108   \n",
       "2  0.131  ...  0.140  0.135  0.134  0.106  0.131  0.169  0.170  0.141  0.163   \n",
       "3  0.137  ...  0.136  0.136  0.304  0.159  0.126  0.145  0.196  0.158  0.173   \n",
       "4  0.141  ...  0.170  0.144  0.098  0.183  0.316  0.190  0.121  0.176  0.158   \n",
       "\n",
       "   hh_47  \n",
       "0  0.126  \n",
       "1  0.093  \n",
       "2  0.137  \n",
       "3  0.149  \n",
       "4  0.146  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process AMI Data\n",
    "# The smart meter data is stored at the half-hour level of granularity in series of CSV files. \n",
    "# Here, we iterate over the files and concatenate into a single data frame in 'wide' format\n",
    "\n",
    "ami_files = os.listdir(data_path + \"/hhblock_dataset/hhblock_dataset/\")\n",
    "\n",
    "# Iterate over files using list comprehension\n",
    "df_from_each_file = (pd.read_csv(data_path + \"/hhblock_dataset/hhblock_dataset/\" + f, sep=',') for f in ami_files)\n",
    "\n",
    "df_merged   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "df_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:21:08.026654Z",
     "start_time": "2021-10-09T00:21:07.777629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visibility</th>\n",
       "      <th>windBearing</th>\n",
       "      <th>temperature</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>dewPoint</th>\n",
       "      <th>pressure</th>\n",
       "      <th>windSpeed</th>\n",
       "      <th>humidity</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.97</td>\n",
       "      <td>104</td>\n",
       "      <td>10.24</td>\n",
       "      <td>2011-11-11 00:00:00</td>\n",
       "      <td>8.86</td>\n",
       "      <td>1016.76</td>\n",
       "      <td>2.77</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2011-11-11</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.88</td>\n",
       "      <td>99</td>\n",
       "      <td>9.76</td>\n",
       "      <td>2011-11-11 01:00:00</td>\n",
       "      <td>8.83</td>\n",
       "      <td>1016.63</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2011-11-11</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.70</td>\n",
       "      <td>98</td>\n",
       "      <td>9.46</td>\n",
       "      <td>2011-11-11 02:00:00</td>\n",
       "      <td>8.79</td>\n",
       "      <td>1016.36</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2011-11-11</td>\n",
       "      <td>02:00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.12</td>\n",
       "      <td>99</td>\n",
       "      <td>9.23</td>\n",
       "      <td>2011-11-11 03:00:00</td>\n",
       "      <td>8.63</td>\n",
       "      <td>1016.28</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2011-11-11</td>\n",
       "      <td>03:00:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.85</td>\n",
       "      <td>111</td>\n",
       "      <td>9.26</td>\n",
       "      <td>2011-11-11 04:00:00</td>\n",
       "      <td>9.21</td>\n",
       "      <td>1015.98</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2011-11-11</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   visibility  windBearing  temperature            timestamp  dewPoint  \\\n",
       "0        5.97          104        10.24  2011-11-11 00:00:00      8.86   \n",
       "1        4.88           99         9.76  2011-11-11 01:00:00      8.83   \n",
       "2        3.70           98         9.46  2011-11-11 02:00:00      8.79   \n",
       "3        3.12           99         9.23  2011-11-11 03:00:00      8.63   \n",
       "4        1.85          111         9.26  2011-11-11 04:00:00      9.21   \n",
       "\n",
       "   pressure  windSpeed  humidity        date      time  hour  \n",
       "0   1016.76       2.77      0.91  2011-11-11  00:00:00     0  \n",
       "1   1016.63       2.95      0.94  2011-11-11  01:00:00     1  \n",
       "2   1016.36       3.17      0.96  2011-11-11  02:00:00     2  \n",
       "3   1016.28       3.25      0.96  2011-11-11  03:00:00     3  \n",
       "4   1015.98       3.70      1.00  2011-11-11  04:00:00     4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process Hourly Weather Data\n",
    "wthr = pd.read_csv(data_path + \"/weather_hourly_darksky.csv\")\n",
    "\n",
    "wthr = wthr.rename(columns = {\"time\":\"timestamp\"})\n",
    "\n",
    "wthr['date'] = pd.to_datetime(wthr['timestamp']).dt.date.astype(str)\n",
    "wthr['time'] = pd.to_datetime(wthr['timestamp']).dt.time.astype(str)\n",
    "wthr['hour'] = pd.to_datetime(wthr['timestamp']).dt.hour.astype(int)\n",
    "\n",
    "wthr = wthr.drop([\"icon\", \"summary\", \"precipType\", \"apparentTemperature\"], axis = 1)\n",
    "wthr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:21:08.056628Z",
     "start_time": "2021-10-09T00:21:08.029654Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time Lookup\n",
    "lookup = pd.read_csv(data_path + \"/time-lookup.csv\", sep = \",\")\n",
    "lookup['timestamp'] = pd.to_datetime(lookup['time'])\n",
    "lookup['time'] = lookup[\"timestamp\"].dt.time\n",
    "lookup['hour'] = lookup[\"timestamp\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:21:08.420040Z",
     "start_time": "2021-10-09T00:21:08.060136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique Number of Sensor meters\n",
    "ids = pd.DataFrame(df_merged['LCLid'].unique())\n",
    "ids.columns = ['id']\n",
    "ids.shape\n",
    "\n",
    "# Simple Random Sample\n",
    "sample_ids = ids.sample(2000)\n",
    "sample_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pivoting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T00:27:11.424565Z",
     "start_time": "2021-10-09T00:21:08.423443Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [05:58<00:00, 89.74s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 500  #chunk row size\n",
    "\n",
    "# Use List Comprehension to create a list of data frame batches\n",
    "list_df = [sample_ids[i:i + batch_size] for i in range(0,sample_ids.shape[0],batch_size)]\n",
    "#list_df = list_df[1]\n",
    "\n",
    "rs = []\n",
    "\n",
    "for i in tqdm(list_df):\n",
    "    try:\n",
    "        local_df = df_merged.loc[df_merged['LCLid'].isin(list_df[1][\"id\"])]\n",
    "\n",
    "        df_long = (pd.wide_to_long(local_df, stubnames = \"hh_\", i = ['day', 'LCLid'], j = \"hh\")\n",
    "            .sort_values([\"day\", \"hh\"])\n",
    "            .reset_index())\n",
    "\n",
    "        df = (df_long.join(lookup.set_index('hh'), how = \"inner\", on = \"hh\")\n",
    "            .rename(columns = {\"day\":\"date\"})\n",
    "            .sort_values([\"date\", \"time\"])\n",
    "            .rename(columns={'hh_':'kw'}))\n",
    "\n",
    "        df = df.drop([\"timestamp\", \"time\", \"hh\"], axis = 1).groupby([\"LCLid\", \"date\", \"hour\"]).agg(kw = (\"kw\", sum)).reset_index()\n",
    "\n",
    "        df = pd.merge(df, wthr.drop([\"time\"], axis = 1), on = [\"date\", \"hour\"])\n",
    "        rs.append(df)\n",
    "        del df_long\n",
    "        del df\n",
    "        del local_df\n",
    "        \n",
    "    except:\n",
    "        print(\"Failed to generate features\")\n",
    "\n",
    "del list_df\n",
    "        \n",
    "import gc\n",
    "gc.collect()\n",
    "        \n",
    "rs = pd.concat(rs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T22:17:52.631361Z",
     "start_time": "2021-10-09T22:16:35.329715Z"
    }
   },
   "outputs": [],
   "source": [
    "#rs.head()\n",
    "\n",
    "#rs.describe()\n",
    "#rs.to_csv(data_path + \"/ami-long.csv\", index = False)\n",
    "\n",
    "df = pd.read_csv(data_path + \"/ami-long.csv\", sep = \",\").sample(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T22:17:53.050430Z",
     "start_time": "2021-10-09T22:17:52.635349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59473"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc as gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T22:17:53.128407Z",
     "start_time": "2021-10-09T22:17:53.054324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hour             int64\n",
       "visibility     float64\n",
       "windBearing      int64\n",
       "temperature    float64\n",
       "dewPoint       float64\n",
       "pressure       float64\n",
       "windSpeed      float64\n",
       "humidity       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "y = df[\"kw\"]\n",
    "X = df.drop([\"LCLid\", \"kw\", \"date\", \"timestamp\"], axis = 1)\n",
    "#del df\n",
    "\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-09T17:53:33.257Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#df.head()\n",
    "#ohe = OneHotEncoder()\n",
    "#ohe.fit(X)\n",
    "X_enc = ohe.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-09T22:16:38.680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f0fd4954d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f0fd4954d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "500000/500000 [==============================] - 1403s 3ms/step - loss: nan - mean_absolute_error: nan\n",
      "Epoch 2/10\n",
      "500000/500000 [==============================] - 1490s 3ms/step - loss: nan - mean_absolute_error: nan\n",
      "Epoch 3/10\n",
      "500000/500000 [==============================] - 1474s 3ms/step - loss: nan - mean_absolute_error: nan\n",
      "Epoch 4/10\n",
      "500000/500000 [==============================] - 1508s 3ms/step - loss: nan - mean_absolute_error: nan\n",
      "Epoch 5/10\n",
      "447891/500000 [=========================>....] - ETA: 2:46 - loss: nan - mean_absolute_error: nan"
     ]
    }
   ],
   "source": [
    "# first neural network with keras tutorial\n",
    "#\n",
    "\n",
    "#from numpy import loadtxt\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "# load the dataset\n",
    "#dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "#X = dataset[:,0:8]\n",
    "#y = dataset[:,8]\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(tf.keras.layers.Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation = \"linear\"))\n",
    "# compile the keras model\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=10, batch_size=2)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "\n",
    "\n",
    "# Define Sequential model with 3 layers\n",
    "# model = keras.Sequential(\n",
    "#     [\n",
    "#         layers.Dense(units = 32, activation= \"relu\", name= \"layer1\"),\n",
    "#         layers.Dense(units = 32, activation= \"relu\", name= \"layer2\"),\n",
    "#         layers.Dense(units = 32, activation= \"relu\", name= \"layer3\"),\n",
    "#         layers.Dense(units = 32, activation= \"relu\", name= \"layer4\"),\n",
    "#         layers.Dense(units = 32, activation= \"relu\", name= \"layer5\"),\n",
    "#         layers.Dense(units = 1, activation = \"linear\")\n",
    "#     ]\n",
    "# )\n",
    "# Call model on a test input\n",
    "#x = tf.ones((3, 3))\n",
    "#y = model(x)\n",
    "\n",
    "\n",
    "\n",
    "# deep_model = kera.\n",
    "#     layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(X_train))) %>%\n",
    "#     layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(X_train))) %>%\n",
    "#     layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(X_train))) %>%\n",
    "#     layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(X_train))) %>%\n",
    "#     layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(X_train))) %>%\n",
    "#     # layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(W))) %>%\n",
    "#     # layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(W))) %>%\n",
    "#     # layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(W))) %>%\n",
    "#     # layer_dropout(0.15) %>%\n",
    "#     #    layer_batch_normalization() %>%\n",
    "#     layer_dense(units = 1, activation = 'linear') %>%\n",
    "#     compile(\n",
    "#       loss =  'mean_absolute_error', #abs_var_penal_loss, #\n",
    "#       optimizer =  optimizer_adam(decay = decay_rate), #optimizer_adam(lr = 0.001, decay = decay_rate), #optimizer_sgd(lr = 0.00001*5, momentum = 0.0, nesterov = FALSE, decay = 0.05/nrow(X_train)),\n",
    "#       metrics = c('mean_absolute_error')\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T17:14:52.420979Z",
     "start_time": "2021-10-09T17:14:52.300639Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Found: <tensorflow.python.keras.layers.core.Dense object at 0x7fb9139c9d10>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5a5a1957cdd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"layer1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"layer2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"layer3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     ]\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise TypeError('The added layer must be '\n\u001b[1;32m    132\u001b[0m                             \u001b[0;34m'an instance of class Layer. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                             'Found: ' + str(layer))\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: <tensorflow.python.keras.layers.core.Dense object at 0x7fb9139c9d10>"
     ]
    }
   ],
   "source": [
    "# Define Sequential model with 3 layers\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2, activation=\"relu\", name=\"layer1\"),\n",
    "        layers.Dense(3, activation=\"relu\", name=\"layer2\"),\n",
    "        layers.Dense(4, name=\"layer3\"),\n",
    "    ]\n",
    ")\n",
    "# Call model on a test input\n",
    "x = tf.ones((3, 3))\n",
    "y = model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-09T00:19:28.460Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process Household Metadata ###\n",
    "\n",
    "#\n",
    "hh_info = pd.read_csv(data_path + \"/acorn_details.csv\", encoding = 'unicode_escape')\n",
    "print(hh_info.head())\n",
    "\n",
    "#\n",
    "hh_info = pd.read_csv(data_path + \"/informations_households.csv\")\n",
    "hh_info[\"Acorn\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T16:58:11.902174Z",
     "start_time": "2021-10-08T16:58:11.892977Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T05:16:27.757552Z",
     "start_time": "2021-10-08T05:14:50.481608Z"
    }
   },
   "outputs": [],
   "source": [
    "#batch_size = 100  #chunk row size\n",
    "\n",
    "# Use List Comprehension to create a list of data frame batches\n",
    "#list_df = [ids[i:i + batch_size] for i in range(0,ids.shape[0],batch_size)]\n",
    "#list_df = list_df[1]\n",
    "\n",
    "df_long = (pd.wide_to_long(local_df, stubnames = \"hh_\", i = ['day', 'LCLid'], j = \"hh\")\n",
    "    .sort_values([\"day\", \"hh\"])\n",
    "    .reset_index())\n",
    "\n",
    "df = (df_long.join(lookup.set_index('hh'), how = \"inner\", on = \"hh\")\n",
    "    .rename(columns = {\"day\":\"date\"})\n",
    "    .sort_values([\"date\", \"time\"])\n",
    "    .rename(columns={'hh_':'kw'}))\n",
    "\n",
    "df = df.drop([\"timestamp\", \"time\", \"hh\"], axis = 1).groupby([\"LCLid\", \"date\", \"hour\"]).agg(kw = (\"kw\", sum)).reset_index()\n",
    "\n",
    "df = pd.merge(df, wthr.drop([\"time\"], axis = 1), on = [\"date\", \"hour\"])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T04:17:56.326572Z",
     "start_time": "2021-10-08T04:17:56.306678Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T00:17:51.474008Z",
     "start_time": "2021-10-03T00:17:51.460469Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_lr_stats(df):\n",
    "    '''function to subset ami data frame, create load research features based on set of ids'''\n",
    "    \n",
    "    # pivot wide to long\n",
    "    df = (pd.wide_to_long(df, stubnames = \"hh_\", i = ['day', 'LCLid'], j = \"hh\")\n",
    "        #.sort_values([\"day\", \"hh\"])\n",
    "        .reset_index())\n",
    "    \n",
    "    # reassign columns\n",
    "    df.columns = ['ds', 'unique_id', 'hh', 'y']\n",
    "\n",
    "    # drop NAs\n",
    "    df = df[[\"ds\", \"unique_id\", \"y\"]].dropna()\n",
    "    \n",
    "    # calculate features\n",
    "    features = df.groupby([\"unique_id\"]).agg([\"max\", \"mean\"])\n",
    "    features['unique_id'] = features.index\n",
    "    \n",
    "    #features = df.head()\n",
    "    return pd.DataFrame(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T00:29:06.048799Z",
     "start_time": "2021-10-03T00:17:52.781737Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1000  #chunk row size\n",
    "\n",
    "# Use List Comprehension to create a list of data frame batches\n",
    "list_df = [ids[i:i + batch_size] for i in range(0,ids.shape[0],batch_size)]\n",
    "\n",
    "rs = []\n",
    "\n",
    "for i in tqdm(list_df):\n",
    "    try:\n",
    "        local_df = df_merged.loc[df_merged['LCLid'].isin(i[\"id\"])]\n",
    "        rs.append(calc_lr_stats(local_df))\n",
    "    except:\n",
    "        print(\"Failed to generate features\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T01:52:59.877194Z",
     "start_time": "2021-10-03T01:52:59.544947Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using Spark\n",
    "# - reference https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\n",
    "# - reference https://docs.faculty.ai/how_to/spark/local_spark.html\n",
    "# - referemce https://spark.apache.org/docs/2.4.4/sql-pyspark-pandas-with-arrow.html\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.conf import SparkConf\n",
    "from functools import reduce\n",
    "import pyspark\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "         .master(\"local[*]\") \\\n",
    "         .appName(\"energy-forecasting\") \\\n",
    "         .config(\"spark.executor.memory\", \"10g\") \\\n",
    "         .config(\"spark.executor.cores\", 8) \\\n",
    "         .config(\"spark.deploy.defaultCores\", 8) \\\n",
    "         .config(\"spark.driver.memory\", \"3g\") \\\n",
    "         .config(\"spark.sql.execution.arrow.pyspark.enabled\", True) \\\n",
    "         .getOrCreate()\n",
    "\n",
    "# explicit functions\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "@pandas_udf(\"max double, mean double, unique_id string\", PandasUDFType.GROUPED_MAP)\n",
    "def calc_lr_stats_py(df):\n",
    "    '''function to subset ami data frame, create load research features based on set of ids'''\n",
    "\n",
    "    # pivot wide to long\n",
    "    df = (pd.wide_to_long(df, stubnames = \"hh_\", i = ['day', 'LCLid'], j = \"hh\")\n",
    "        #.sort_values([\"day\", \"hh\"])\n",
    "        .reset_index())\n",
    "    \n",
    "    # reassign columns\n",
    "    df.columns = ['ds', 'unique_id', 'hh', 'y']\n",
    "\n",
    "    # drop NAs\n",
    "    df = df[[\"ds\", \"unique_id\", \"y\"]].dropna()\n",
    "    \n",
    "    # calculate features\n",
    "    features = df.groupby([\"unique_id\"]).agg([\"max\", \"mean\"])\n",
    "    features['unique_id'] = features.index\n",
    "    \n",
    "    #features = df.head()\n",
    "    return pd.DataFrame(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T00:17:44.124253Z",
     "start_time": "2021-10-03T00:17:44.099086Z"
    }
   },
   "outputs": [],
   "source": [
    "rs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-03T01:53:27.598Z"
    }
   },
   "outputs": [],
   "source": [
    "### Use Batch Processing and Pandas UDF in Spark to create Load Research Statistics ###\n",
    "%time\n",
    "\n",
    "batch_size = ids.shape[0]  #chunk row size\n",
    "\n",
    "# Use List Comprehension to create a list of data frame batches\n",
    "list_df = [ids[i:i + batch_size] for i in range(0,ids.shape[0],batch_size)]\n",
    "\n",
    "#gc.collect()\n",
    "rs = []\n",
    "\n",
    "for i in tqdm(list_df):\n",
    "    try:\n",
    "        local_df = df_merged.loc[df_merged['LCLid'].isin(i[\"id\"])]\n",
    "        spark_df = spark.createDataFrame(local_df)\n",
    "        print(\"Copied Data Into Spark...\")\n",
    "        rs_df = spark_df.groupby('LCLid').apply(calc_lr_stats_py).collect()\n",
    "        #rs_df = spark_df.groupby('LCLid').apply(calc_lr_stats_py)\n",
    "        rs.append(rs_df.select(\"*\").toPandas())\n",
    "\n",
    "    except:\n",
    "        print(\"Failed to generate features\")\n",
    "    \n",
    "rs = pd.concat(rs)\n",
    "\n",
    "#unionAll(*[df1, df2, df3]).show()\n",
    "\n",
    "# reassign columns\n",
    "rs.columns = ['peak_demand', 'avg_demand', 'unique_id']\n",
    "rs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T03:00:01.513137Z",
     "start_time": "2021-10-02T03:00:01.480879Z"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "#num_df = rs[['peak_demand', 'avg_demand']]\n",
    "#num_df[\"load_factor\"] = num_df['avg_demand'].div(num_df['peak_demand']).replace(np.inf, 0)\n",
    "#num_df = num_df.dropna()\n",
    "\n",
    "\n",
    "# K-means clustering\n",
    "#num_df[\"cluster\"] = KMeans(n_clusters = 3, random_state = 0).fit_predict(num_df)\n",
    "#num_df[\"cluster\"] = num_df[\"cluster\"].astype(str)\n",
    "#num_df[\"cluster\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T03:47:01.481942Z",
     "start_time": "2021-10-01T03:47:00.049873Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "sns.scatterplot(data=num_df, x=\"peak_demand\", y=\"avg_demand\", hue = num_df.cluster.tolist())\n",
    "\n",
    "#sns.lmplot(data=num_df), x=\"peak_demand\", y=\"avg_demand\", hue = \"cluster\", markers =['v', 'o', 'x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T19:52:34.719233Z",
     "start_time": "2021-10-01T19:52:34.221258Z"
    }
   },
   "outputs": [],
   "source": [
    "#sns.scatterplot(data=num_df, x=\"avg_demand\", y=\"peak_demand\", hue = num_df.cluster.tolist())\n",
    "#num_df[\"load_factor\"] = num_df['avg_demand'].div(num_df['peak_demand']).replace(np.inf, 0)\n",
    "\n",
    "fig = plt.figure(figsize = (16, 8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "x = num_df[\"peak_demand\"]\n",
    "y = num_df[\"avg_demand\"]\n",
    "z = num_df[\"load_factor\"]\n",
    "\n",
    "ax.scatter3D(x, y, z)\n",
    "\n",
    "# Add x, y gridlines\n",
    "ax.grid(b = True, color ='grey',\n",
    "        linestyle ='-.', linewidth = 0.3,\n",
    "        alpha = 0.2)\n",
    "\n",
    "my_cmap = plt.get_cmap('Set1')\n",
    " \n",
    "\n",
    "    # Creating plot\n",
    "sctt = ax.scatter3D(x, y, z,\n",
    "                    alpha = 0.7,\n",
    "                    c = num_df[\"cluster\"].astype(int),\n",
    "                    cmap = my_cmap,\n",
    "                    marker ='o')\n",
    "\n",
    " \n",
    "plt.title(\"Load Factor\")\n",
    "ax.set_xlabel('Peak Demand (kW)', fontweight ='bold')\n",
    "ax.set_ylabel('Average Demand (kW)', fontweight ='bold')\n",
    "ax.set_zlabel('Load Factor ', fontweight ='bold')\n",
    "fig.colorbar(sctt, ax = ax, shrink = 0.5, aspect = 5)\n",
    " \n",
    "ax.view_init(20, 320)\n",
    "    \n",
    "# show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Features\n",
    "\n",
    "In addition to classic load research statistics, we can also borrow from other domains that analyze time series data for forecasting purposes. There has been significant research recently into time-series features, and an excellent R package for generating said features is [tsfeatures](https://cran.r-project.org/web/packages/tsfeatures/vignettes/tsfeatures.html). Here we use a [Python implementation / API](https://github.com/Nixtla/tsfeatures) to the R package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T03:13:11.496949Z",
     "start_time": "2021-10-02T03:13:09.612070Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_features_py(df):\n",
    "\n",
    "    # pivot wide to long\n",
    "    df = (pd.wide_to_long(df, stubnames = \"hh_\", i = ['day', 'LCLid'], j = \"hh\")\n",
    "        .sort_values([\"day\", \"hh\"])\n",
    "        .reset_index())\n",
    "    \n",
    "    df.columns = ['ds', 'unique_id', 'hh', 'y']\n",
    "\n",
    "    df = df[[\"ds\", \"unique_id\", \"y\"]].dropna()\n",
    "    \n",
    "    # calculate ts_features\n",
    "    features = tsfeatures.tsfeatures(df, freq = 1, features = [stl_features])\n",
    "    #features = tsfeatures.tsfeatures(df, freq = 1, features = [stl_features, acf_features, entropy, crossing_points, stability, nonlinearity])\n",
    "    #features = tsfeatures.tsfeatures(df, freq = 1)\n",
    "\n",
    "    return pd.DataFrame(features).drop([\"nperiods\", \"seasonal_period\"], axis = 1)\n",
    "\n",
    "check = calc_features_py(df = df_merged)\n",
    "check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T22:27:54.072493Z",
     "start_time": "2021-10-02T22:27:54.031540Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@pandas_udf(\"unique_id string, trend double, spike double, linearity double, curvature double, e_acf1 double, e_acf10 double\", PandasUDFType.GROUPED_MAP)\n",
    "def calc_entropy_features_py(df):\n",
    "    '''function to subset ami data frame, create load research features based on set of ids'''\n",
    "\n",
    "    # pivot wide to long\n",
    "    df = (pd.wide_to_long(df, stubnames = \"hh_\", i = ['day', 'LCLid'], j = \"hh\")\n",
    "        .sort_values([\"day\", \"hh\"])\n",
    "        .reset_index())\n",
    "    \n",
    "    df.columns = ['ds', 'unique_id', 'hh', 'y']\n",
    "\n",
    "    df = df[[\"ds\", \"unique_id\", \"y\"]].dropna()\n",
    "    \n",
    "    # calculate ts_features\n",
    "    features = tsfeatures.tsfeatures(df, freq = 1, features = [entropy, crossing_points, stability, nonlinearity])\n",
    "\n",
    "    return pd.DataFrame(features).drop([\"nperiods\", \"seasonal_period\"], axis = 1)\n",
    "\n",
    "@pandas_udf(\"unique_id string, trend double, spike double, linearity double, curvature double, e_acf1 double, e_acf10 double\", PandasUDFType.GROUPED_MAP)\n",
    "def calc_stl_features_py(df):\n",
    "    '''function to subset ami data frame, create load research features based on set of ids'''\n",
    "\n",
    "    # pivot wide to long\n",
    "    df = (pd.wide_to_long(df, stubnames = \"hh_\", i = ['day', 'LCLid'], j = \"hh\")\n",
    "        .sort_values([\"day\", \"hh\"])\n",
    "        .reset_index())\n",
    "    \n",
    "    df.columns = ['ds', 'unique_id', 'hh', 'y']\n",
    "\n",
    "    df = df[[\"ds\", \"unique_id\", \"y\"]].dropna()\n",
    "    \n",
    "    # calculate ts_features\n",
    "    features = tsfeatures.tsfeatures(df, freq = 1, features = [stl_features])\n",
    "\n",
    "    return pd.DataFrame(features).drop([\"nperiods\", \"seasonal_period\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T22:39:14.099381Z",
     "start_time": "2021-10-02T22:28:02.829267Z"
    }
   },
   "outputs": [],
   "source": [
    "### Use Batch Processing and Pandas UDF in Spark to create Time Series Statistics ###\n",
    "\n",
    "batch_size = 50  #chunk row size\n",
    "\n",
    "# Use List Comprehension to create a list of data frame batches\n",
    "list_df = [ids[i:i + batch_size] for i in range(0,ids.shape[0],batch_size)]\n",
    "\n",
    "gc.collect()\n",
    "rs = []\n",
    "\n",
    "for i in tqdm(list_df):\n",
    "    try:\n",
    "        local_df = df_merged.loc[df_merged['LCLid'].isin(i[\"id\"])]\n",
    "        spark_df = spark.createDataFrame(local_df)\n",
    "        rs_df = spark_df.groupby('LCLid').apply(calc_stl_features_py).collect()\n",
    "        rs.append(pd.DataFrame(rs_df))\n",
    "\n",
    "    except:\n",
    "        print(\"Failed to generate features\")\n",
    "    \n",
    "rs = pd.concat(rs)\n",
    "    \n",
    "# reassign columns\n",
    "#rs.columns = ['peak_demand', 'avg_demand', 'unique_id']\n",
    "rs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 10  #chunk row size\n",
    "\n",
    "# Use List Comprehension to create a list of data frame batches\n",
    "list_df = [ids[i:i + batch_size] for i in range(0,ids.shape[0],batch_size)]\n",
    "\n",
    "rs = []\n",
    "\n",
    "for i in tqdm(list_df):\n",
    "    print(i)\n",
    "    try:\n",
    "        rs.append(calc_features(i, df = df_merged))\n",
    "    except:\n",
    "        print(\"Failed to generate features\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - local models\n",
    "# - global models\n",
    "# - local models with clusters \"ensembles of exprts\"\n",
    "\n",
    "\n",
    "# Local Methods\n",
    "\n",
    "\n",
    "# Global Models\n",
    "def train_gbm\n",
    "\n",
    "def train_mlp\n",
    "\n",
    "def train_\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datastar",
   "language": "python",
   "name": "datastar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
