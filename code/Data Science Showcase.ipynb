{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Energy Data Science Experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "Being able to accurately forecast the volatile demand for power, especially across low-voltage distribution grids, is increasingly important to providing sufficent energy supply. Paradoxically, it's at the low-voltage networks that are the most volatile, and yet the most critical to account for as more distributed energy resources (e.g., solar, storage) make their way onto the grid. From the grid operator's point of view, not only are lower voltage service points (e.g., customers) the revenue life blood of the business, having increased visibility into demand at such lower levels of aggregation helps to provide \n",
    "\n",
    "\n",
    "In this notebook, I explore looking at a Kaggle data set containing about 5,000 utility service points with time series data from advanced metering infrastructure (AMI). The context of this experiment is to simulate the needs of a utility or grid operator and generate accurate short-term (up to 7 days ahead) forecast. I explore possible approaches that involve\n",
    "\n",
    "- *Feature-based modeling approaches*\n",
    "- *Customer Segmentation + Clustering*\n",
    "- *Statistical methods, ML methods, and hybrid cross-learning approaches*\n",
    "\n",
    "Performance is evaluated using time-series cross-validation using a rolling 7-day forecast horizon.\n",
    "\n",
    "Finally, I show how this solution might be scaled and put into production.\n",
    "\n",
    "Keywords: *time series classification*, *energy forecasting*, *cross-learning*, *MLOps*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T19:21:01.785851Z",
     "start_time": "2021-08-20T19:21:01.762127Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotnine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-92327c373688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mplotnine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotnine\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotnine'"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "\n",
    "### Utilities ###\n",
    "\n",
    "import os\n",
    "import glob\n",
    "#import boto3\n",
    "import s3fs\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Data Processing ###\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "### PLOTTING ###\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "\n",
    "import seaborn as sns\n",
    "from plotnine import *\n",
    "import plotnine as pl\n",
    "\n",
    "### DATA SCIENCE ###\n",
    "\n",
    "#import lightgbm as lgb\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix,classification_report\n",
    "from sklearn.model_selection import GridSearchCV,KFold,RandomizedSearchCV\n",
    "from scipy.stats import uniform, truncnorm, randint\n",
    "import random\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Project Working Directory\n",
    "project_path = \"/Users/jonfarland/Dropbox/08-Projects/showcase/\"\n",
    "\n",
    "#os.getcwd()\n",
    "os.chdir(project_path)\n",
    "\n",
    "#os.environ\n",
    "#pd.set_option('display.max_columns', None)\n",
    "\n",
    "data_path = f\"{project_path}data\"\n",
    "data_path\n",
    " \n",
    "# Read in Secure Credentials from Local File\n",
    "#with open('cred.json') as f:\n",
    "#  cred = json.load(f)\n",
    "\n",
    "# Set AWS Credentials\n",
    "#os.environ[\"AWS_ACCESS_KEY_ID\"] = cred['aws'].get(\"AWS_ACCESS_KEY_ID\")\n",
    "#os.environ[\"AWS_SECRET_ACCESS_KEY\"] = cred['aws'].get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "# Set Twillio Credentials\n",
    "#os.environ[\"TWILIO_SID\"] = cred['twilio'].get(\"cred1\")\n",
    "#os.environ[\"TWILIO_TOKEN\"] = cred['twilio'].get(\"cred2\")\n",
    "#os.environ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "\n",
    "#https://www.kaggle.com/jeanmidev/smart-meters-in-london\n",
    "\n",
    "#hh = pd.read_csv(data_path + \"halfhourly_dataset\")\n",
    "all_files = os.listdir(\"data/archive/hhblock_dataset/hhblock_dataset\")\n",
    "\n",
    "df_from_each_file = (pd.read_csv(data_path + \"/archive/hhblock_dataset/hhblock_dataset/\" + f, sep=',') for f in all_files)\n",
    "df_merged   = pd.concat(df_from_each_file, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged.shape)\n",
    "print(df_merged.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df_merged[\"day\"], df_merged[\"hh_0\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_merged[df_merged['LCLid'] == 'MAC000027']\n",
    "#df.head()\n",
    "\n",
    "## Tranpose from wide to long\n",
    "\n",
    "df = pd.wide_to_long(df_merged, stubnames = \"hh_\", i = ['day', 'LCLid'], j = \"hh\").sort_values([\"day\", \"hh\"]).reset_index()\n",
    "\n",
    "#df.columns.values\n",
    "#df.shape\n",
    "\n",
    "#ax.plot(df0[\"hh_\"])\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df[\"hh\"], df[\"hh_\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time lookup\n",
    "lookup = pd.read_csv(data_path + \"/time-lookup.csv\", sep = \",\")\n",
    "lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values([\"LCLid\", \"day\", \"hh\"])\n",
    "df = df.join(lookup.set_index(\"hh\"), how = \"left\", on = \"hh\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"timestamp\"] = pd.to_datetime(df['day'] + \" \" + df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"day\", \"hh\", \"time\"], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"id\", \"kwh\", \"timestamp\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide = df.pivot(index = \"timestamp\", columns = 'id', values = 'kwh')\n",
    "\n",
    "df_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide = df_wide.to_numpy()\n",
    "df_wide[np.isnan(df_wide)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_wide.shape)\n",
    "print(np.transpose(df_wide).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = np.transpose(df_wide)\n",
    "X_train, y_train = series[:4500, :39695], series[:4500, -1]\n",
    "X_valid, y_valid = series[4500:5000, :39695], series[4500:5000, -1]\n",
    "X_test, y_test = series[5000:, :39695], series[5000:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train[~np.isnan(X_train)], y_train[~np.isnan(y_train)]\n",
    "X_valid, y_valid = X_valid[~np.isnan(X_valid)], y_valid[~np.isnan(y_valid)]\n",
    "X_test, y_test = X_test[~np.isnan(X_test)], y_test[~np.isnan(y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_valid[:, -1]\n",
    "np.mean(keras.losses.mean_squared_error(y_valid[~np.isnan(y_valid)], y_pred[~np.isnan(y_pred)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[39695, 1]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(loss, val_loss):\n",
    "    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.-\", label=\"Training loss\")\n",
    "    plt.plot(np.arange(len(val_loss)) + 1, val_loss, \"r.-\", label=\"Validation loss\")\n",
    "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "    plt.axis([1, 100, 0, 30])\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_valid)\n",
    "plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Number of Sensor meters\n",
    "ids = pd.DataFrame(df_merged['LCLid'].unique()).sample(n = 1000)\n",
    "ids.columns = ['id']\n",
    "\n",
    "df = df.loc[df['LCLid'].isin(ids[\"id\"])]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_merged[df_merged['LCLid'] == 'MAC000027']\n",
    "#df.head()\n",
    "\n",
    "df0 = pd.wide_to_long(df_merged, stubnames = \"hh_\", i = ['day', 'LCLid'], j = \"hh\").sort_values([\"day\", \"hh\"])\n",
    "df = df0.reset_index()\n",
    "\n",
    "#df.columns.values\n",
    "df.shape\n",
    "\n",
    "#ax.plot(df0[\"hh_\"])\n",
    "#fig, ax = plt.subplots()\n",
    "#ax.scatter(df[\"hh\"], df[\"hh_\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide = df.pivot(index = \"timestamp\", columns = 'id', values = 'usage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "### User Defined Functions ###\n",
    "##############################\n",
    "\n",
    "# Define a function to recursively flatten JSON object\n",
    "def flatten_json(y):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(y)\n",
    "    return out\n",
    "\n",
    "# Define a function to process each JSON record\n",
    "def process_row(x):\n",
    "    input_json = json.loads(x)\n",
    "    flat_json = flatten_json(input_json)\n",
    "    flat_df = pd.DataFrame.from_dict(flat_json, orient = \"index\").T\n",
    "    return flat_df\n",
    "\n",
    "# Define a function process an entire file\n",
    "def process_file(filename, destination = \"data/processed/\"):\n",
    "\tdf = pd.read_excel(filename)\n",
    "\n",
    "\t# Extract Participant ID\n",
    "\tpid = df.participant_id.iloc[0]\n",
    "\n",
    "\t# Split data based on data category\n",
    "\tops_data = df[df['data_category'] == 'Operational']\n",
    "\tconf_data = df[df['data_category'] == 'Configuration']\n",
    "\n",
    "\t# Use List Comprehension to apply function to each row\n",
    "\tlist_output = [process_row(x) for x in tqdm(ops_data['hvac'])]\n",
    "\n",
    "\t# Concatenate list output to produce data frame\n",
    "\tdf_output = pd.concat(list_output, ignore_index = True)\n",
    "\n",
    "\t# Concatenate Processed JSON with IDs\n",
    "\toutput = pd.concat((ops_data.drop(['hvac'], axis = 1).reset_index(), df_output), axis=1)\n",
    "\n",
    "\t# Establish Output File Name\n",
    "\toutput_file_name = destination + pid + '-ops.csv'\n",
    "\n",
    "\t# Writeout operational data\n",
    "\toutput.to_csv(output_file_name)\n",
    "\n",
    "\t# Use List Comprehension to apply function to each row\n",
    "\tlist_output = [process_row(x) for x in tqdm(conf_data['hvac'])]\n",
    "\n",
    "\t# Concatenate list output to produce data frame\n",
    "\tdf_output = pd.concat(list_output, ignore_index = True)\n",
    "\n",
    "\t# Concatenate Processed JSON with IDs\n",
    "\toutput = pd.concat((conf_data.drop(['hvac'], axis = 1).reset_index(), df_output), axis=1)\n",
    "\n",
    "\t# Establish Output File Name\n",
    "\toutput_file_name = destination + pid + '-config.csv'\n",
    "\n",
    "\t# Writeout configuration data data\n",
    "\toutput.to_csv(output_file_name)\n",
    "\n",
    "\n",
    "# list files in current directory\n",
    "directory = os.getcwd()\n",
    "\n",
    "# iterate over files in current directory and process them\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".xlsx\"):\n",
    "        try:\n",
    "        \tprint(\"Processing File: \" + filename)\n",
    "        \tprocess_file(filename, destination = \"/Users/jonfarland/Documents/projects/APC/data/processed/\")\n",
    "        except:\n",
    "        \tprint(\"Error Processing File: \" + filename)\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datastar",
   "language": "python",
   "name": "datastar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
